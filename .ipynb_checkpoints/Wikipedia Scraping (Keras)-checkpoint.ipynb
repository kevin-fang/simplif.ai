***REMOVED***
 "cells": [
  ***REMOVED***
   "cell_type": "code",
   "execution_count": 5,
   "metadata": ***REMOVED***
    "collapsed": true
   ***REMOVED***,
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")"
   ]
  ***REMOVED***,
  ***REMOVED***
   "cell_type": "code",
   "execution_count": 6,
   "metadata": ***REMOVED***
    "collapsed": true
   ***REMOVED***,
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "with open('articles.json') as f:\n",
    "    data = json.load(f)"
   ]
  ***REMOVED***,
  ***REMOVED***
   "cell_type": "code",
   "execution_count": 7,
   "metadata": ***REMOVED******REMOVED***,
   "outputs": [
    ***REMOVED***
     "data": ***REMOVED***
      "text/plain": [
       "['Donald_Trump',\n",
       " 'United_States',\n",
       " 'Bitcoin',\n",
       " 'Queen_Victoria',\n",
       " 'Elon_Musk',\n",
       " 'Facebook',\n",
       " 'Barack_Obama',\n",
       " 'YouTube',\n",
       " 'Aristotle',\n",
       " 'Statistics',\n",
       " 'Chemistry',\n",
       " 'Monkey',\n",
       " 'Chess',\n",
       " 'Python_(programming_language)',\n",
       " 'Comedy',\n",
       " 'Trial',\n",
       " 'Executive_(government)',\n",
       " 'State',\n",
       " 'Human_rights',\n",
       " 'Art',\n",
       " 'Social_capital',\n",
       " 'Parasitism',\n",
       " 'Wikipedia',\n",
       " 'Document',\n",
       " 'Book',\n",
       " 'Composer',\n",
       " 'Classical_music',\n",
       " 'Serenade',\n",
       " 'Computer',\n",
       " 'President',\n",
       " 'Business',\n",
       " 'Law',\n",
       " 'Government',\n",
       " 'Prison',\n",
       " 'Medicine',\n",
       " 'Disease',\n",
       " 'History']"
      ]
     ***REMOVED***,
     "execution_count": 7,
     "metadata": ***REMOVED******REMOVED***,
     "output_type": "execute_result"
    ***REMOVED***
   ],
   "source": [
    "articles = data['articles']\n",
    "articles"
   ]
  ***REMOVED***,
  ***REMOVED***
   "cell_type": "code",
   "execution_count": 8,
   "metadata": ***REMOVED***
    "collapsed": true
   ***REMOVED***,
   "outputs": [],
   "source": [
    "page_py = wiki.page(articles[0])"
   ]
  ***REMOVED***,
  ***REMOVED***
   "cell_type": "code",
   "execution_count": 9,
   "metadata": ***REMOVED***
    "collapsed": true
   ***REMOVED***,
   "outputs": [],
   "source": [
    "def save_sections(sections, lst, level=0):\n",
    "        for s in sections:\n",
    "            skip = [\"References\", \"Sources\", \"Further reading\", \"External links\", \"See also\", \"Other websites\"]\n",
    "            if not s.title in skip:\n",
    "                lst.append(s.text.strip())\n",
    "                save_sections(s.sections, lst, level + 1)"
   ]
  ***REMOVED***,
  ***REMOVED***
   "cell_type": "code",
   "execution_count": 10,
   "metadata": ***REMOVED***
    "collapsed": true
   ***REMOVED***,
   "outputs": [],
   "source": [
    "def get_simplified_page(wiki):\n",
    "    return wiki.langlinks['simple']"
   ]
  ***REMOVED***,
  ***REMOVED***
   "cell_type": "code",
   "execution_count": 11,
   "metadata": ***REMOVED******REMOVED***,
   "outputs": [
    ***REMOVED***
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Donald_Trump\n",
      "Finished: United_States\n",
      "Finished: Bitcoin\n",
      "Finished: Queen_Victoria\n",
      "Finished: Elon_Musk\n",
      "Finished: Facebook\n",
      "Finished: Barack_Obama\n",
      "Finished: YouTube\n",
      "Finished: Aristotle\n",
      "Finished: Statistics\n",
      "Finished: Chemistry\n",
      "Finished: Monkey\n",
      "Finished: Chess\n",
      "Finished: Python_(programming_language)\n",
      "Finished: Comedy\n",
      "Finished: Trial\n",
      "Finished: Executive_(government)\n",
      "Finished: State\n",
      "Finished: Human_rights\n",
      "Finished: Art\n",
      "Finished: Social_capital\n",
      "Finished: Parasitism\n",
      "Finished: Wikipedia\n",
      "Finished: Document\n",
      "Finished: Book\n",
      "Finished: Composer\n",
      "Finished: Classical_music\n",
      "Finished: Serenade\n",
      "Finished: Computer\n",
      "Finished: President\n",
      "Finished: Business\n",
      "Finished: Law\n",
      "Finished: Government\n",
      "Finished: Prison\n",
      "Finished: Medicine\n",
      "Finished: Disease\n",
      "Finished: History\n"
     ]
    ***REMOVED***
   ],
   "source": [
    "all_complex = []\n",
    "all_simple = []\n",
    "for article in articles:\n",
    "    \n",
    "    # scrape wikipedia page\n",
    "    page = wiki.page(article)\n",
    "    complex_sections = []\n",
    "    save_sections(page.sections, complex_sections)\n",
    "    \n",
    "    simple_sections = []\n",
    "    save_sections(get_simplified_page(page).sections, simple_sections)\n",
    "    \n",
    "    # delete extra text and make it parseable\n",
    "    complex_sentences = \". \".join(complex_sections) \\\n",
    "        .replace(\"\\n\", \" \") \\\n",
    "        .replace('\"', '\\'') \\\n",
    "        .split(\". \")\n",
    "\n",
    "    complex_sentences = map(lambda x: x.strip(), complex_sentences)\n",
    "    complex_sentences = filter(lambda x: x != '', complex_sentences)\n",
    "    complex_sentences = list(complex_sentences)\n",
    "\n",
    "    simple_sentences = \". \".join(simple_sections) \\\n",
    "        .replace(\"\\n\", \" \") \\\n",
    "        .replace('\"', '\\'') \\\n",
    "        .split('. ')\n",
    "\n",
    "    simple_sentences = list(map(lambda x: x.strip(), simple_sentences))\n",
    "    simple_sentences = filter(lambda x: x != '', simple_sentences)\n",
    "    simple_sentences = list(simple_sentences)\n",
    "    \n",
    "    all_complex.extend(complex_sentences)\n",
    "    all_simple.extend(simple_sentences)\n",
    "#    print(complex_sentences)\n",
    "    print(\"Finished: ***REMOVED******REMOVED***\".format(article))"
   ]
  ***REMOVED***,
  ***REMOVED***
   "cell_type": "code",
   "execution_count": 12,
   "metadata": ***REMOVED******REMOVED***,
   "outputs": [
    ***REMOVED***
     "data": ***REMOVED***
      "text/plain": [
       "10279"
      ]
     ***REMOVED***,
     "execution_count": 12,
     "metadata": ***REMOVED******REMOVED***,
     "output_type": "execute_result"
    ***REMOVED***
   ],
   "source": [
    "len(all_complex)"
   ]
  ***REMOVED***,
  ***REMOVED***
   "cell_type": "code",
   "execution_count": 13,
   "metadata": ***REMOVED***
    "scrolled": true
   ***REMOVED***,
   "outputs": [
    ***REMOVED***
     "data": ***REMOVED***
      "text/plain": [
       "1944"
      ]
     ***REMOVED***,
     "execution_count": 13,
     "metadata": ***REMOVED******REMOVED***,
     "output_type": "execute_result"
    ***REMOVED***
   ],
   "source": [
    "len(all_simple)"
   ]
  ***REMOVED***,
  ***REMOVED***
   "cell_type": "code",
   "execution_count": 14,
   "metadata": ***REMOVED***
    "collapsed": true
   ***REMOVED***,
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  ***REMOVED***,
  ***REMOVED***
   "cell_type": "code",
   "execution_count": 28,
   "metadata": ***REMOVED***
    "collapsed": true
   ***REMOVED***,
   "outputs": [],
   "source": [
    "with open('complex.txt', 'w') as file:\n",
    "    for i in all_complex[:len(all_simple)]:\n",
    "        file.write(i.lower() + '.||')\n",
    "with open('simple.txt', 'w') as file:\n",
    "    for i in all_simple:\n",
    "        file.write(i.lower() + '.||')"
   ]
  ***REMOVED***,
  ***REMOVED***
   "cell_type": "code",
   "execution_count": 19,
   "metadata": ***REMOVED******REMOVED***,
   "outputs": [
    ***REMOVED***
     "data": ***REMOVED***
      "text/plain": [
       "'Therefore, the performance of the PCA model is evaluated using computer\\\\[Hyphen]based resampling techniques such as the bootstrap and cross\\\\[Hyphen]validation techniques where the data are separated into a learning and a testing set. '"
      ]
     ***REMOVED***,
     "execution_count": 19,
     "metadata": ***REMOVED******REMOVED***,
     "output_type": "execute_result"
    ***REMOVED***
   ],
   "source": [
    "lemmatizer.lemmatize(\"Therefore, the performance of the PCA model is evaluated using computer\\[Hyphen]based resampling techniques such as the bootstrap and cross\\[Hyphen]validation techniques where the data are separated into a learning and a testing set. \")"
   ]
  ***REMOVED***,
  ***REMOVED***
   "cell_type": "code",
   "execution_count": 23,
   "metadata": ***REMOVED******REMOVED***,
   "outputs": [
    ***REMOVED***
     "data": ***REMOVED***
      "text/plain": [
       "'good'"
      ]
     ***REMOVED***,
     "execution_count": 23,
     "metadata": ***REMOVED******REMOVED***,
     "output_type": "execute_result"
    ***REMOVED***
   ],
   "source": [
    "lemmatizer.lemmatize('better', pos='a')"
   ]
  ***REMOVED***,
  ***REMOVED***
   "cell_type": "code",
   "execution_count": 24,
   "metadata": ***REMOVED***
    "collapsed": true
   ***REMOVED***,
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, PunktSentenceTokenizer"
   ]
  ***REMOVED***,
  ***REMOVED***
   "cell_type": "code",
   "execution_count": 25,
   "metadata": ***REMOVED***
    "collapsed": true
   ***REMOVED***,
   "outputs": [],
   "source": [
    "tok = sent_tokenize(\"Therefore, the performance of the PCA model is evaluated using computer\\[Hyphen]based resampling techniques such as the bootstrap and cross\\[Hyphen]validation techniques where the data are separated into a learning and a testing set. \")"
   ]
  ***REMOVED***,
  ***REMOVED***
   "cell_type": "code",
   "execution_count": 26,
   "metadata": ***REMOVED******REMOVED***,
   "outputs": [
    ***REMOVED***
     "data": ***REMOVED***
      "text/plain": [
       "['Therefore, the performance of the PCA model is evaluated using computer\\\\[Hyphen]based resampling techniques such as the bootstrap and cross\\\\[Hyphen]validation techniques where the data are separated into a learning and a testing set.']"
      ]
     ***REMOVED***,
     "execution_count": 26,
     "metadata": ***REMOVED******REMOVED***,
     "output_type": "execute_result"
    ***REMOVED***
   ],
   "source": [
    "tok"
   ]
  ***REMOVED***,
  ***REMOVED***
   "cell_type": "code",
   "execution_count": null,
   "metadata": ***REMOVED***
    "collapsed": true
   ***REMOVED***,
   "outputs": [],
   "source": []
  ***REMOVED***
 ],
 "metadata": ***REMOVED***
  "kernelspec": ***REMOVED***
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  ***REMOVED***,
  "language_info": ***REMOVED***
   "codemirror_mode": ***REMOVED***
    "name": "ipython",
    "version": 3
   ***REMOVED***,
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  ***REMOVED***
 ***REMOVED***,
 "nbformat": 4,
 "nbformat_minor": 2
***REMOVED***
